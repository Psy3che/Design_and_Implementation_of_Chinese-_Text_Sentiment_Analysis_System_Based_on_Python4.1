{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T16:09:33.356182Z",
     "start_time": "2025-03-17T16:09:33.287839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "# # 一、导入数据\n",
    "# # -- coding: utf-8 --\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import jieba\n",
    "# import torch\n",
    "# import warnings\n",
    "# from torch.optim import AdamW\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "# import os\n",
    "#\n",
    "# warnings.filterwarnings(\"ignore\")  # 忽略警告信息\n",
    "#\n",
    "# # 加载语料库文件，并导入数据\n",
    "# neg = pd.read_excel('data/neg.xls', header=None)\n",
    "# pos = pd.read_excel('data/pos.xls', header=None)\n",
    "#\n",
    "# pos.head()\n",
    "#\n",
    "# # 分词处理\n",
    "# word_cut = lambda x: jieba.lcut(str(x))\n",
    "# pos['words'] = pos[0].apply(word_cut)\n",
    "# neg['words'] = neg[0].apply(word_cut)\n",
    "#\n",
    "# # 使用 1 表示积极情绪，0 表示消极情绪，并完成数组拼接\n",
    "# texts = np.concatenate((pos['words'], neg['words']))\n",
    "# labels = np.concatenate((np.ones(len(pos)), np.zeros(len(neg))))\n",
    "#\n",
    "# # 二、使用 BERT 进行情感分类\n",
    "# # 加载预训练的 BERT 模型和分词器\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=2)\n",
    "#\n",
    "# # 将分词后的句子转换为 BERT 的输入格式\n",
    "# def encode_text(texts):\n",
    "#     return tokenizer(texts,\n",
    "#                      return_tensors='pt',\n",
    "#                      padding=True,\n",
    "#                      truncation=True,\n",
    "#                      max_length=128)\n",
    "#\n",
    "# # 准备训练数据\n",
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "#\n",
    "# train_encodings = encode_text([' '.join(text) for text in train_texts])\n",
    "# val_encodings = encode_text([' '.join(text) for text in val_texts])\n",
    "#\n",
    "# # 将数据转换为 PyTorch 数据集\n",
    "# class SentimentDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "#\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}  # 确保索引为整数类型\n",
    "#         item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "#         return item\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "#\n",
    "# train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "# val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "#\n",
    "#\n",
    "# # 自定义计算 F1 值的函数\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=1)\n",
    "#     f1 = f1_score(labels, predictions, average='binary')  # 二分类问题使用 'binary'\n",
    "#     return {\"f1\": f1}\n",
    "#\n",
    "# # def compute_metrics(eval_pred):\n",
    "# #     logits, labels = eval_pred\n",
    "# #     predictions = np.argmax(logits, axis=1)\n",
    "# #     accuracy = accuracy_score(labels, predictions)\n",
    "# #     precision = precision_score(labels, predictions, average='binary')\n",
    "# #     recall = recall_score(labels, predictions, average='binary')\n",
    "# #     f1 = f1_score(labels, predictions, average='binary')\n",
    "# #     return {\n",
    "# #         \"accuracy\": accuracy,\n",
    "# #         \"precision\": precision,\n",
    "# #         \"recall\": recall,\n",
    "# #         \"f1\": f1\n",
    "# #     }\n",
    "# # 设置优化器和学习率调整器\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)  # 初始学习率\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True, min_lr=1e-7)\n",
    "#\n",
    "# # 三、训练 BERT 情感分类模型\n",
    "# # 设置训练参数\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     warmup_steps=500,\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",  # 每个 epoch 结束时进行评估\n",
    "#     logging_dir='./logs',\n",
    "#     lr_scheduler_type=\"reduce_lr_on_plateau\",  # 使用 ReduceLROnPlateau 调度器\n",
    "#     metric_for_best_model=\"loss\",  # 使用损失值作为最佳模型的评估指标\n",
    "# )\n",
    "# # training_args = TrainingArguments(\n",
    "# #     output_dir='./results',\n",
    "# #     num_train_epochs=3,\n",
    "# #     per_device_train_batch_size=16,\n",
    "# #     per_device_eval_batch_size=64,\n",
    "# #     warmup_steps=500,\n",
    "# #     weight_decay=0.01,\n",
    "# #     evaluation_strategy=\"epoch\",  # 每个 epoch 结束时进行评估\n",
    "# #     logging_dir='./logs',\n",
    "# #     lr_scheduler_type=\"reduce_lr_on_plateau\",  # 使用 ReduceLROnPlateau 调度器\n",
    "# #     metric_for_best_model=\"f1\",  # 使用 F1 Score 作为最佳模型的评估指标\n",
    "# # )\n",
    "# # 初始化 Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "#\n",
    "# # 开始训练\n",
    "# trainer.train()\n",
    "#\n",
    "# # 输出训练后的指标\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(\"Training Results:\")\n",
    "# # print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "# # print(f\"Precision: {eval_results['eval_precision']:.4f}\")\n",
    "# # print(f\"Recall: {eval_results['eval_recall']:.4f}\")\n",
    "# print(f\"F1 Score: {eval_results['eval_f1']:.4f}\")\n",
    "#\n",
    "# # 创建表格并打印\n",
    "# df = pd.DataFrame([eval_results])\n",
    "# print(\"\\nEvaluation Metrics:\")\n",
    "# print(df.to_markdown(index=False))\n",
    "# # 保存训练好的模型\n",
    "# model.save_pretrained('bert_sentiment_model')\n",
    "# tokenizer.save_pretrained('bert_sentiment_model')\n",
    "#\n",
    "# # 四、情感预测\n",
    "# # 加载训练好的模型\n",
    "# model = BertForSequenceClassification.from_pretrained('bert_sentiment_model')\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert_sentiment_model')\n",
    "#\n",
    "# # 检查设备\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "#\n",
    "# # 对电影评论进行情感判断\n",
    "# def bert_predict(string):\n",
    "#     # 对输入文本进行编码\n",
    "#     inputs = tokenizer(string, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}  # 确保输入张量在正确的设备上\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         predicted_class = torch.argmax(logits).item()\n",
    "#\n",
    "#     # 输出结果\n",
    "#     sentiment = '积极' if predicted_class == 1 else '消极'\n",
    "#     print(f\"{string} [{sentiment}]\")\n",
    "#     return sentiment\n",
    "#\n",
    "# # 测试预测\n",
    "# string = '还不错，符合需求'\n",
    "# pred_result = bert_predict(string)\n",
    "# print(f\"预测结果: {pred_result}\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T16:09:33.483061Z",
     "start_time": "2025-03-17T16:09:33.477496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import jieba\n",
    "# import torch\n",
    "# from torch.optim import AdamW\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "# import  warnings\n",
    "# import os\n",
    "# from sklearn.metrics import classification_report, f1_score\n",
    "#\n",
    "# warnings.filterwarnings(\"ignore\")  # 忽略警告信息\n",
    "#\n",
    "# # 数据加载和预处理\n",
    "# neg = pd.read_excel('data/neg.xls', header=None)\n",
    "# pos = pd.read_excel('data/pos.xls', header=None)\n",
    "#\n",
    "# pos['words'] = pos[0].apply(lambda x: jieba.lcut(str(x)))\n",
    "# neg['words'] = neg[0].apply(lambda x: jieba.lcut(str(x)))\n",
    "#\n",
    "# texts = np.concatenate((pos['words'], neg['words']))\n",
    "# labels = np.concatenate((np.ones(len(pos)), np.zeros(len(neg))))\n",
    "#\n",
    "# # BERT 模型准备\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=2)\n",
    "#\n",
    "# def encode_text(texts):\n",
    "#     return tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "#\n",
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "#\n",
    "# train_encodings = encode_text([' '.join(text) for text in train_texts])\n",
    "# val_encodings = encode_text([' '.join(text) for text in val_texts])\n",
    "#\n",
    "# class SentimentDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "#\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "#         return item\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "#\n",
    "# train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "# val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "#\n",
    "# # 计算多个指标\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=1)\n",
    "#\n",
    "#     report = classification_report(labels, predictions, output_dict=True)\n",
    "#     accuracy = report['accuracy']\n",
    "#     precision = report['macro avg']['precision']\n",
    "#     recall = report['macro avg']['recall']\n",
    "#     f1 = report['macro avg']['f1-score']\n",
    "#\n",
    "#     return {\n",
    "#         \"accuracy\": accuracy,\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall,\n",
    "#         \"f1\": f1,\n",
    "#         \"loss\": float('nan'),  # 这里可能需要根据具体需求调整\n",
    "#     }\n",
    "#\n",
    "# # 设置优化器和学习率调整器\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True, min_lr=1e-7)\n",
    "#\n",
    "# # 训练参数\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     warmup_steps=500,\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     logging_dir='./logs',\n",
    "#     lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "#     metric_for_best_model=\"loss\",\n",
    "# )\n",
    "#\n",
    "# # 初始化 Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "#\n",
    "# # 开始训练\n",
    "# trainer.train()\n",
    "#\n",
    "# # 保存模型\n",
    "# model.save_pretrained('bert_sentiment_model')\n",
    "# tokenizer.save_pretrained('bert_sentiment_model')\n",
    "#\n",
    "# # 输出训练指标表格\n",
    "# log_history = trainer.state.log_history\n",
    "#\n",
    "# # 初始化一个列表来保存每一epoch的指标\n",
    "# metrics = []\n",
    "#\n",
    "# for log_item in log_history:\n",
    "#     # 确保该日志条目包含训练或评估的指标\n",
    "#     if 'loss' in log_item or 'eval_loss' in log_item:\n",
    "#         # 记录 epoch，如果有的话\n",
    "#         epoch = log_item.get('epoch', None)\n",
    "#         # 获取 Training Loss\n",
    "#         train_loss = log_item.get('loss', None)\n",
    "#         # 获取 Validation Loss\n",
    "#         eval_loss = log_item.get('eval_loss', None)\n",
    "#         # 获取其他指标\n",
    "#         accuracy = log_item.get('eval_accuracy', None)\n",
    "#         precision = log_item.get('eval_precision', None)\n",
    "#         recall = log_item.get('eval_recall', None)\n",
    "#         f1 = log_item.get('eval_f1', None)\n",
    "#\n",
    "#         # 把当前 Epoch 的指标收集到字典中\n",
    "#         metrics.append({\n",
    "#             'cust-epoch': epoch,\n",
    "#             'train_loss': train_loss,\n",
    "#             'eval_loss': eval_loss,\n",
    "#             'Accuracy': accuracy,\n",
    "#             'Precision': precision,\n",
    "#             'Recall': recall,\n",
    "#             'F1 Score': f1\n",
    "#         })\n",
    "#\n",
    "# # 把列表转换为 Pandas DataFrame\n",
    "# metrics_df = pd.DataFrame(metrics)\n",
    "#\n",
    "# # 使用 pd.set_option 来设置表格显示的列宽和宽度，避免省略号\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "#\n",
    "# print(\"Model training metrics table:\")\n",
    "# print(metrics_df)\n",
    "#\n",
    "# # 情感预测部分\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "#\n",
    "# def bert_predict(string):\n",
    "#     inputs = tokenizer(string, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         predicted_class = torch.argmax(logits).item()\n",
    "#     sentiment = '积极' if predicted_class == 1 else '消极'\n",
    "#     print(f\"{string} [{sentiment}]\")\n",
    "#     return sentiment\n",
    "#\n",
    "# string = '还不错，符合需求'\n",
    "# pred_result = bert_predict(string)\n",
    "# print(f\"预测结果: {pred_result}\")"
   ],
   "id": "7b534c65d0ea2330",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T16:09:33.574899Z",
     "start_time": "2025-03-17T16:09:33.570798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import jieba\n",
    "# import torch\n",
    "# from torch.optim import AdamW\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# import warnings\n",
    "# import os\n",
    "#\n",
    "# warnings.filterwarnings(\"ignore\")  # 忽略警告信息\n",
    "#\n",
    "# # 数据加载和预处理\n",
    "# neg = pd.read_excel('data/neg.xls', header=None)\n",
    "# pos = pd.read_excel('data/pos.xls', header=None)\n",
    "#\n",
    "# pos['words'] = pos[0].apply(lambda x: jieba.lcut(str(x)))\n",
    "# neg['words'] = neg[0].apply(lambda x: jieba.lcut(str(x)))\n",
    "#\n",
    "# texts = np.concatenate((pos['words'], neg['words']))\n",
    "# labels = np.concatenate((np.ones(len(pos)), np.zeros(len(neg))))\n",
    "#\n",
    "# # BERT 模型准备\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=2)\n",
    "#\n",
    "# def encode_text(texts):\n",
    "#     return tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "#\n",
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "#\n",
    "# train_encodings = encode_text([' '.join(text) for text in train_texts])\n",
    "# val_encodings = encode_text([' '.join(text) for text in val_texts])\n",
    "#\n",
    "# class SentimentDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "#\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "#         return item\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "#\n",
    "# train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "# val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "#\n",
    "# # 计算多个指标\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=1)\n",
    "#\n",
    "#     accuracy = accuracy_score(labels, predictions)\n",
    "#     precision = precision_score(labels, predictions, average='binary')\n",
    "#     recall = recall_score(labels, predictions, average='binary')\n",
    "#     f1 = f1_score(labels, predictions, average='binary')\n",
    "#\n",
    "#     return {\n",
    "#         \"accuracy\": accuracy,\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall,\n",
    "#         \"f1\": f1,\n",
    "#     }\n",
    "#\n",
    "# # 设置优化器和学习率调整器\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True, min_lr=1e-7)\n",
    "#\n",
    "# # 训练参数\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     warmup_steps=500,\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     logging_dir='./logs',\n",
    "#     lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "#     metric_for_best_model=\"accuracy\",  # 使用准确率作为最佳模型的评估指标\n",
    "# )\n",
    "#\n",
    "# # 初始化 Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "#\n",
    "# # 开始训练\n",
    "# trainer.train()\n",
    "#\n",
    "# # 保存模型\n",
    "# model.save_pretrained('bert_sentiment_model')\n",
    "# tokenizer.save_pretrained('bert_sentiment_model')\n",
    "#\n",
    "# # 输出训练指标表格\n",
    "# log_history = trainer.state.log_history\n",
    "#\n",
    "# # 初始化一个列表来保存每一epoch的指标\n",
    "# metrics = []\n",
    "#\n",
    "# for log_item in log_history:\n",
    "#     # 确保该日志条目包含训练或评估的指标\n",
    "#     if 'loss' in log_item or 'eval_loss' in log_item:\n",
    "#         # 记录 epoch，如果有的话\n",
    "#         epoch = log_item.get('epoch', None)\n",
    "#         # 获取 Training Loss\n",
    "#         train_loss = log_item.get('loss', None)\n",
    "#         # 获取 Validation Loss\n",
    "#         eval_loss = log_item.get('eval_loss', None)\n",
    "#         # 获取其他指标\n",
    "#         accuracy = log_item.get('eval_accuracy', None)\n",
    "#         precision = log_item.get('eval_precision', None)\n",
    "#         recall = log_item.get('eval_recall', None)\n",
    "#         f1 = log_item.get('eval_f1', None)\n",
    "#\n",
    "#         # 把当前 Epoch 的指标收集到字典中\n",
    "#         metrics.append({\n",
    "#             'cust-epoch': epoch,\n",
    "#             'train_loss': train_loss,\n",
    "#             'eval_loss': eval_loss,\n",
    "#             'Accuracy': accuracy,\n",
    "#             'Precision': precision,\n",
    "#             'Recall': recall,\n",
    "#             'F1 Score': f1\n",
    "#         })\n",
    "#\n",
    "# # 把列表转换为 Pandas DataFrame\n",
    "# metrics_df = pd.DataFrame(metrics)\n",
    "#\n",
    "# # 使用 pd.set_option 来设置表格显示的列宽和宽度，避免省略号\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "#\n",
    "# print(\"Model training metrics table:\")\n",
    "# print(metrics_df)\n",
    "#\n",
    "# # 情感预测部分\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "#\n",
    "# def bert_predict(string):\n",
    "#     inputs = tokenizer(string, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         predicted_class = torch.argmax(logits).item()\n",
    "#     sentiment = '积极' if predicted_class == 1 else '消极'\n",
    "#     print(f\"{string} [{sentiment}]\")\n",
    "#     return sentiment\n",
    "#\n",
    "# string = '还不错，符合需求'\n",
    "# pred_result = bert_predict(string)\n",
    "# print(f\"预测结果: {pred_result}\")"
   ],
   "id": "7d4dfc6b7732d5e1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T17:04:14.460262Z",
     "start_time": "2025-03-17T16:54:00.590812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # 忽略警告信息\n",
    "\n",
    "# 数据加载和预处理\n",
    "# 假设每个情感类别对应一个txt文件，文件名分别为：like.txt、dislike.txt、happy.txt、sad.txt、angry.txt、surprised.txt、afraid.txt、positive.txt、negative.txt、neutral.txt\n",
    "# 每个文件中包含对应情感的文本数据，每行一个文本\n",
    "\n",
    "# 定义情感类别和对应的文件名\n",
    "emotions = ['喜欢', '厌恶', '开心', '难过', '愤怒', '惊讶', '害怕', '积极', '消极', '中性']\n",
    "file_names = ['like.txt', 'dislike.txt', 'happy.txt', 'sad.txt', 'angry.txt', 'surprised.txt', 'afraid.txt', 'positive.txt', 'negative.txt', 'neutral.txt']\n",
    "\n",
    "# 加载数据\n",
    "texts = []\n",
    "labels = []\n",
    "for idx, file_name in enumerate(file_names):\n",
    "    with open(os.path.join('data', file_name), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                texts.append(line)\n",
    "                labels.append(idx)\n",
    "\n",
    "# 将文本进行分词处理\n",
    "texts = [jieba.lcut(text) for text in texts]\n",
    "\n",
    "# BERT 模型准备\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=len(emotions))\n",
    "\n",
    "def encode_text(texts):\n",
    "    return tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# 对文本进行编码\n",
    "train_encodings = encode_text([' '.join(text) for text in train_texts])\n",
    "val_encodings = encode_text([' '.join(text) for text in val_texts])\n",
    "\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "\n",
    "# 计算多个指标\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='macro')\n",
    "    recall = recall_score(labels, predictions, average='macro')\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "# 设置优化器和学习率调整器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True, min_lr=1e-7)\n",
    "\n",
    "# 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "    metric_for_best_model=\"accuracy\",  # 使用准确率作为最佳模型的评估指标\n",
    ")\n",
    "\n",
    "# 初始化 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained('bert_sentiment_model')\n",
    "tokenizer.save_pretrained('bert_sentiment_model')\n",
    "\n",
    "# 输出训练指标表格\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# 初始化一个列表来保存每一epoch的指标\n",
    "metrics = []\n",
    "\n",
    "for log_item in log_history:\n",
    "    # 确保该日志条目包含训练或评估的指标\n",
    "    if 'loss' in log_item or 'eval_loss' in log_item:\n",
    "        # 记录 epoch，如果有的话\n",
    "        epoch = log_item.get('epoch', None)\n",
    "        # 获取 Training Loss\n",
    "        train_loss = log_item.get('loss', None)\n",
    "        # 获取 Validation Loss\n",
    "        eval_loss = log_item.get('eval_loss', None)\n",
    "        # 获取其他指标\n",
    "        accuracy = log_item.get('eval_accuracy', None)\n",
    "        precision = log_item.get('eval_precision', None)\n",
    "        recall = log_item.get('eval_recall', None)\n",
    "        f1 = log_item.get('eval_f1', None)\n",
    "\n",
    "        # 把当前 Epoch 的指标收集到字典中\n",
    "        metrics.append({\n",
    "            'cust-epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'eval_loss': eval_loss,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1\n",
    "        })\n",
    "\n",
    "# 把列表转换为 Pandas DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# 使用 pd.set_option 来设置表格显示的列宽和宽度，避免省略号\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"Model training metrics table:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# 情感预测部分\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def bert_predict(string):\n",
    "    inputs = tokenizer(string, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits).item()\n",
    "    sentiment = emotions[predicted_class]\n",
    "    print(f\"{string} [{sentiment}]\")\n",
    "    return sentiment\n",
    "\n",
    "string = '在这个复杂多变的社会里，成年人之间的友谊似乎越来越像一场精心策划的交易。它不再是小时候那种纯粹的、毫无保留的情感连接，而是建立在利益、需求和相互利用基础上的一种微妙平衡。'\n",
    "pred_result = bert_predict(string)\n",
    "print(f\"预测结果: {pred_result}\")"
   ],
   "id": "b340b61522b2bcfb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/grace/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/_w/hfqvgqcd4rj79tq5flh6k91h0000gn/T/jieba.cache\n",
      "Loading model cost 0.315 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 10:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.593145</td>\n",
       "      <td>0.421000</td>\n",
       "      <td>0.437085</td>\n",
       "      <td>0.424197</td>\n",
       "      <td>0.398872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.554100</td>\n",
       "      <td>1.558200</td>\n",
       "      <td>0.451000</td>\n",
       "      <td>0.440333</td>\n",
       "      <td>0.457524</td>\n",
       "      <td>0.431725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.554100</td>\n",
       "      <td>1.610294</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.457239</td>\n",
       "      <td>0.449866</td>\n",
       "      <td>0.444297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training metrics table:\n",
      "   cust-epoch  train_loss  eval_loss  Accuracy  Precision    Recall  F1 Score\n",
      "0         1.0         NaN   1.593145     0.421   0.437085  0.424197  0.398872\n",
      "1         2.0      1.5541        NaN       NaN        NaN       NaN       NaN\n",
      "2         2.0         NaN   1.558200     0.451   0.440333  0.457524  0.431725\n",
      "3         3.0         NaN   1.610294     0.452   0.457239  0.449866  0.444297\n",
      "在这个复杂多变的社会里，成年人之间的友谊似乎越来越像一场精心策划的交易。它不再是小时候那种纯粹的、毫无保留的情感连接，而是建立在利益、需求和相互利用基础上的一种微妙平衡。 [消极]\n",
      "预测结果: 消极\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T03:00:55.820957Z",
     "start_time": "2025-03-18T02:56:22.836390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # 忽略警告信息\n",
    "\n",
    "# 数据加载和预处理\n",
    "# 假设数据集保存在train.txt文件中，格式为JSON格式，每行一个样本\n",
    "# 样本格式：{\"id\": 21, \"content\": \"4…这都不是偶然的事情，这是他明白这个事实真相，对生死看得很淡薄，知道要认真努力修行，所以死了之后，自己可以作主，自己可以选择到哪一道去。如果在临终时，一慌一乱，你对身体很留恋，对家亲眷属很留恋，那你往往就到三途去了！\", \"label\": \"fear\"}\n",
    "\n",
    "# 定义情感类别和对应的标签映射\n",
    "emotions = ['like', 'disgust', 'happiness', 'sadness', 'anger', 'fear']\n",
    "label_to_id = {label: idx for idx, label in enumerate(emotions)}\n",
    "\n",
    "# 加载数据\n",
    "texts = []\n",
    "labels = []\n",
    "with open('data/train.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        text = data['content']\n",
    "        label = data['label']\n",
    "        if label in label_to_id:\n",
    "            texts.append(text)\n",
    "            labels.append(label_to_id[label])\n",
    "\n",
    "# 将文本进行分词处理\n",
    "texts = [jieba.lcut(text) for text in texts]\n",
    "\n",
    "# BERT 模型准备\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=len(emotions))\n",
    "\n",
    "def encode_text(texts):\n",
    "    return tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# 对文本进行编码\n",
    "train_encodings = encode_text([' '.join(text) for text in train_texts])\n",
    "val_encodings = encode_text([' '.join(text) for text in val_texts])\n",
    "\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "\n",
    "# 计算多个指标\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='macro')\n",
    "    recall = recall_score(labels, predictions, average='macro')\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "# 设置优化器和学习率调整器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True, min_lr=1e-7)\n",
    "\n",
    "# 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "    metric_for_best_model=\"accuracy\",  # 使用准确率作为最佳模型的评估指标\n",
    ")\n",
    "\n",
    "# 初始化 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained('bert_sentiment_model')\n",
    "tokenizer.save_pretrained('bert_sentiment_model')\n",
    "\n",
    "# 输出训练指标表格\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# 初始化一个列表来保存每一epoch的指标\n",
    "metrics = []\n",
    "\n",
    "for log_item in log_history:\n",
    "    # 确保该日志条目包含训练或评估的指标\n",
    "    if 'loss' in log_item or 'eval_loss' in log_item:\n",
    "        # 记录 epoch，如果有的话\n",
    "        epoch = log_item.get('epoch', None)\n",
    "        # 获取 Training Loss\n",
    "        train_loss = log_item.get('loss', None)\n",
    "        # 获取 Validation Loss\n",
    "        eval_loss = log_item.get('eval_loss', None)\n",
    "        # 获取其他指标\n",
    "        accuracy = log_item.get('eval_accuracy', None)\n",
    "        precision = log_item.get('eval_precision', None)\n",
    "        recall = log_item.get('eval_recall', None)\n",
    "        f1 = log_item.get('eval_f1', None)\n",
    "\n",
    "        # 把当前 Epoch 的指标收集到字典中\n",
    "        metrics.append({\n",
    "            'cust-epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'eval_loss': eval_loss,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1\n",
    "        })\n",
    "\n",
    "# 把列表转换为 Pandas DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# 使用 pd.set_option 来设置表格显示的列宽和宽度，避免省略号\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"Model training metrics table:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# 情感预测部分\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def bert_predict(string):\n",
    "    inputs = tokenizer(string, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits).item()\n",
    "    sentiment = emotions[predicted_class]\n",
    "    print(f\"{string} [{sentiment}]\")\n",
    "    return sentiment\n",
    "\n",
    "string = '还不错，符合需求'\n",
    "pred_result = bert_predict(string)\n",
    "print(f\"预测结果: {pred_result}\")"
   ],
   "id": "d40c27076cf93b48",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='4638' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 108/4638 03:55 < 2:47:46, 0.45 it/s, Epoch 0.07/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 114\u001B[0m\n\u001B[1;32m    105\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m    106\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m    107\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    110\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics,\n\u001B[1;32m    111\u001B[0m )\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# 开始训练\u001B[39;00m\n\u001B[0;32m--> 114\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;66;03m# 保存模型\u001B[39;00m\n\u001B[1;32m    117\u001B[0m model\u001B[38;5;241m.\u001B[39msave_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert_sentiment_model\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:2171\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2169\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2170\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2172\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2176\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:2531\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2524\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2525\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2526\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2527\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[1;32m   2528\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2529\u001B[0m )\n\u001B[1;32m   2530\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2531\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2533\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2534\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2535\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2536\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2537\u001B[0m ):\n\u001B[1;32m   2538\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2539\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:3678\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3676\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss(model, inputs)\n\u001B[1;32m   3677\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3678\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3680\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[1;32m   3681\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   3682\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3683\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   3684\u001B[0m ):\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:3734\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3732\u001B[0m         loss_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_items_in_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_items_in_batch\n\u001B[1;32m   3733\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mloss_kwargs}\n\u001B[0;32m-> 3734\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3735\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   3736\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   3737\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:1665\u001B[0m, in \u001B[0;36mBertForSequenceClassification.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1657\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1658\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1659\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[1;32m   1660\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[1;32m   1661\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[1;32m   1662\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1663\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1665\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1666\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1668\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1670\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1671\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1672\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1673\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1674\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1675\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1677\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   1679\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(pooled_output)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:1142\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1135\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[1;32m   1136\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[1;32m   1137\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[1;32m   1138\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[1;32m   1139\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[1;32m   1140\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m-> 1142\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1143\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1144\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1145\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1146\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1147\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1148\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1149\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1150\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1151\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1152\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1153\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1154\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1155\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:695\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    684\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    685\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    686\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    692\u001B[0m         output_attentions,\n\u001B[1;32m    693\u001B[0m     )\n\u001B[1;32m    694\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 695\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    696\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    698\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    699\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    700\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    701\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    703\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    706\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:627\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    624\u001B[0m     cross_attn_present_key_value \u001B[38;5;241m=\u001B[39m cross_attention_outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    625\u001B[0m     present_key_value \u001B[38;5;241m=\u001B[39m present_key_value \u001B[38;5;241m+\u001B[39m cross_attn_present_key_value\n\u001B[0;32m--> 627\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m \u001B[43mapply_chunking_to_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed_forward_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchunk_size_feed_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseq_len_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_output\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    630\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (layer_output,) \u001B[38;5;241m+\u001B[39m outputs\n\u001B[1;32m    632\u001B[0m \u001B[38;5;66;03m# if decoder, return the attn key/values as the last output\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pytorch_utils.py:255\u001B[0m, in \u001B[0;36mapply_chunking_to_forward\u001B[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[1;32m    252\u001B[0m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[1;32m    253\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(output_chunks, dim\u001B[38;5;241m=\u001B[39mchunk_dim)\n\u001B[0;32m--> 255\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:639\u001B[0m, in \u001B[0;36mBertLayer.feed_forward_chunk\u001B[0;34m(self, attention_output)\u001B[0m\n\u001B[1;32m    638\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfeed_forward_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, attention_output):\n\u001B[0;32m--> 639\u001B[0m     intermediate_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintermediate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    640\u001B[0m     layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(intermediate_output, attention_output)\n\u001B[1;32m    641\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:540\u001B[0m, in \u001B[0;36mBertIntermediate.forward\u001B[0;34m(self, hidden_states)\u001B[0m\n\u001B[1;32m    538\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m    539\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense(hidden_states)\n\u001B[0;32m--> 540\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintermediate_act_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/activations.py:78\u001B[0m, in \u001B[0;36mGELUActivation.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "baa6e3df17104f38"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
