{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:18:40.142592Z",
     "start_time": "2025-02-20T02:15:41.699620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 一、导入数据\n",
    "# -- coding: utf-8 --\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import torch\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # 忽略警告信息\n",
    "\n",
    "# 加载语料库文件，并导入数据\n",
    "neg = pd.read_excel('data/neg.xls', header=None)\n",
    "pos = pd.read_excel('data/pos.xls', header=None)\n",
    "\n",
    "pos.head()\n",
    "\n",
    "# 分词处理\n",
    "word_cut = lambda x: jieba.lcut(str(x))\n",
    "pos['words'] = pos[0].apply(word_cut)\n",
    "neg['words'] = neg[0].apply(word_cut)\n",
    "\n",
    "# 使用 1 表示积极情绪，0 表示消极情绪，并完成数组拼接\n",
    "texts = np.concatenate((pos['words'], neg['words']))\n",
    "labels = np.concatenate((np.ones(len(pos)), np.zeros(len(neg))))\n",
    "\n",
    "# 准备训练数据\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, train_size=0.1)\n",
    "\n",
    "# 二、构建 Bi-LSTM 模型\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # 双向 LSTM 的输出维度是 hidden_dim * 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "\n",
    "        # Fully connected layer\n",
    "        # 取最后一个时间步的输出\n",
    "        output = self.fc(lstm_out[:, -1, :])  # (batch_size, num_classes)\n",
    "        return output\n",
    "\n",
    "# 三、数据预处理\n",
    "# 构建词汇表\n",
    "vocab = set()\n",
    "for text in texts:\n",
    "    vocab.update(text)\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab) + 1  # 加1是为了留出一个索引给填充符\n",
    "\n",
    "# 将文本转换为索引\n",
    "def text_to_indices(text):\n",
    "    return [vocab.index(word) + 1 for word in text]  # 加1是为了避免索引为0\n",
    "\n",
    "train_indices = [text_to_indices(text) for text in train_texts]\n",
    "val_indices = [text_to_indices(text) for text in val_texts]\n",
    "\n",
    "# 填充序列\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = pad_sequence([torch.tensor(text, dtype=torch.long) for text in texts], batch_first=True)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts, labels\n",
    "\n",
    "# 将数据转换为 PyTorch 数据集\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, indices, labels):\n",
    "        self.indices = indices\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.indices[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SentimentDataset(train_indices, train_labels)\n",
    "val_dataset = SentimentDataset(val_indices, val_labels)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# 四、训练 Bi-LSTM 模型\n",
    "# 设置超参数\n",
    "embed_dim = 64  # 减小嵌入维度\n",
    "hidden_dim = 128  # 减小隐藏层维度\n",
    "num_classes = 2\n",
    "learning_rate = 2e-3\n",
    "\n",
    "# 初始化模型、优化器和学习率调度器\n",
    "device = torch.device(\"cpu\")  # 切换到 CPU\n",
    "model = BiLSTM(vocab_size, embed_dim, hidden_dim, num_classes).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True, min_lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练函数\n",
    "def train(model, train_loader, val_loader, epochs=10):\n",
    "    best_loss = float('inf')\n",
    "    history = []  # 用于存储每个 epoch 的指标\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for texts, labels in val_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                outputs = model(texts)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "        # 计算验证集上的指标\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        precision = precision_score(all_labels, all_predictions, average='binary')\n",
    "        recall = recall_score(all_labels, all_predictions, average='binary')\n",
    "        f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "\n",
    "        # 保存指标到历史记录\n",
    "        history.append({\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Training Loss\": total_loss / len(train_loader),\n",
    "            \"Validation Loss\": val_loss / len(val_loader),\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Validation Loss: {val_loss / len(val_loader):.4f}, \"\n",
    "              f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, \"\n",
    "              f\"Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'bi_lstm_model.pth')\n",
    "            print(\"Best model saved!\")\n",
    "\n",
    "        # 调整学习率\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # 创建表格并打印\n",
    "    df = pd.DataFrame(history)\n",
    "    print(\"\\nTraining History:\")\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "# 开始训练\n",
    "train(model, train_loader, val_loader, epochs=3)\n",
    "\n",
    "# 五、情感预测\n",
    "# 加载训练好的模型\n",
    "model.load_state_dict(torch.load('bi_lstm_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# 对电影评论进行情感判断\n",
    "def bi_lstm_predict(string):\n",
    "    # 对输入文本进行分词\n",
    "    words = jieba.lcut(str(string))\n",
    "    indices = text_to_indices(words)\n",
    "    indices = torch.tensor(indices).unsqueeze(0).to(device)  # 添加 batch 维度\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(indices)\n",
    "        _, predicted_class = torch.max(outputs, 1)\n",
    "\n",
    "    # 输出结果\n",
    "    sentiment = '积极' if predicted_class.item() == 1 else '消极'\n",
    "    print(f\"{string} [{sentiment}]\")\n",
    "    return sentiment\n",
    "\n",
    "# 测试预测\n",
    "string = '还不错，符合需求'\n",
    "pred_result = bi_lstm_predict(string)\n",
    "print(f\"预测结果: {pred_result}\")"
   ],
   "id": "399aabe407a63ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.6879\n",
      "Epoch 1, Validation Loss: 0.6876, Accuracy: 0.5384, Precision: 0.6858, Recall: 0.1624, F1 Score: 0.2626\n",
      "Best model saved!\n",
      "Epoch 2, Training Loss: 0.6920\n",
      "Epoch 2, Validation Loss: 0.6811, Accuracy: 0.5741, Precision: 0.7085, Recall: 0.2695, F1 Score: 0.3905\n",
      "Best model saved!\n",
      "Epoch 3, Training Loss: 0.6793\n",
      "Epoch 3, Validation Loss: 0.6839, Accuracy: 0.5090, Precision: 0.5079, Recall: 0.9588, F1 Score: 0.6641\n",
      "\n",
      "Training History:\n",
      " Epoch  Training Loss  Validation Loss  Accuracy  Precision   Recall  F1 Score\n",
      "     1       0.687855         0.687609  0.538370   0.685771 0.162377  0.262580\n",
      "     2       0.692025         0.681118  0.574135   0.708487 0.269537  0.390508\n",
      "     3       0.679287         0.683928  0.509000   0.507933 0.958821  0.664074\n",
      "还不错，符合需求 [积极]\n",
      "预测结果: 积极\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ff96282d3a86d87b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
